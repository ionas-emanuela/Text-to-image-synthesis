{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text to Image Synthesis",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMFTs1t04uuoRx5LONrpv/j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ionas-emanuela/Text-to-image-synthesis/blob/master/Text_to_Image_Synthesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPTkHQ-ma8ak",
        "colab_type": "code",
        "outputId": "16c9e988-02d3-4adc-bc03-bf4687c8bdb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "!pip install torch===1.2.0 torchvision===0.4.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip3 install torchvision\n",
        "!pip install tensorboardX"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch===1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/57/d5cceb0799c06733eefce80c395459f28970ebb9e896846ce96ab579a3f1/torch-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (748.8MB)\n",
            "\u001b[K     |████████████████████████████████| 748.9MB 22kB/s \n",
            "\u001b[?25hCollecting torchvision===0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/e6/a564eba563f7ff53aa7318ff6aaa5bd8385cbda39ed55ba471e95af27d19/torchvision-0.4.0-cp36-cp36m-manylinux1_x86_64.whl (8.8MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8MB 110kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch===1.2.0) (1.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision===0.4.0) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision===0.4.0) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.5.0+cu101\n",
            "    Uninstalling torch-1.5.0+cu101:\n",
            "      Successfully uninstalled torch-1.5.0+cu101\n",
            "  Found existing installation: torchvision 0.6.0+cu101\n",
            "    Uninstalling torchvision-0.6.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.6.0+cu101\n",
            "Successfully installed torch-1.2.0 torchvision-0.4.0\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
            "Requirement already satisfied: torch==1.2.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (46.3.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NITN6j-vqu2I",
        "colab_type": "code",
        "outputId": "369362cf-9ace-4492-9a7a-a8b10a124462",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!pip install image"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: image in /usr/local/lib/python3.6/dist-packages (1.5.31)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from image) (1.12.0)\n",
            "Requirement already satisfied: django in /usr/local/lib/python3.6/dist-packages (from image) (3.0.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from image) (7.0.0)\n",
            "Requirement already satisfied: asgiref~=3.2 in /usr/local/lib/python3.6/dist-packages (from django->image) (3.2.7)\n",
            "Requirement already satisfied: sqlparse>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from django->image) (0.3.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from django->image) (2018.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfXa9AxzolLf",
        "colab_type": "code",
        "outputId": "932764ab-bd73-4dda-8b35-f78703d6cae1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!pip install utils"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting utils\n",
            "  Downloading https://files.pythonhosted.org/packages/55/e6/c2d2b2703e7debc8b501caae0e6f7ead148fd0faa3c8131292a599930029/utils-1.0.1-py2.py3-none-any.whl\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp5f8K4Zemq-",
        "colab_type": "text"
      },
      "source": [
        "All the necessary imports are here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjPlq0qvzEQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl3lKMrjefYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn, optim\n",
        "from torch.autograd.variable import Variable\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import errno\n",
        "import torchvision.utils as vutils\n",
        "from tensorboardX import SummaryWriter\n",
        "from IPython import display\n",
        "import PIL.Image as Image\n",
        "from matplotlib import pyplot as plt\n",
        "import utils\n",
        "\n",
        "import random\n",
        "\n",
        "from numpy import expand_dims\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy import asarray\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from keras.datasets.fashion_mnist import load_data\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Concatenate\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sJJAfCZbNAo",
        "colab_type": "text"
      },
      "source": [
        "Install the necessary libraries for connecting to google drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAd6XC23bKGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_fZ7ZgkbLHy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_n26L_AbvXe",
        "colab_type": "text"
      },
      "source": [
        "The attributes are taken from here [list_attr_celeba.txt](https://drive.google.com/open?id=0B7EVK8r0v71pblRyaVFSWGxPY0U).\n",
        "They are presented as a table of 41 x 202599. The columns are the image_id + attributes and each row has a value of 1 or -1 (if the attribute is found or not in the image).\n",
        "\n",
        "---\n",
        "\n",
        "The attributes are the following (in this exact order) : ['image_id', '5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open','Mustache', 'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace', 'Wearing_Necktie', 'Young']\n",
        "\n",
        "---\n",
        "\n",
        "Note that these values are saved in a DataFrame, so the indexing is done using .loc['image_id'], this will retrieve the row of data corresponding to a certain image.\n",
        "\n",
        "---\n",
        "The one-hot encoding vector obtained from the textual input is stored in a DataFrame of 40 x 1, the only occupied row being row 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVgZHLgtbq0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_annotation():\n",
        "  downloaded = drive.CreateFile({'id':'0B7EVK8r0v71pblRyaVFSWGxPY0U'})\n",
        "  downloaded.GetContentFile('list_attr_celeba.txt')\n",
        "  rfile = open('list_attr_celeba.txt', 'r')\n",
        "  texts = rfile.read().split(\"\\n\")\n",
        "  rfile.close()\n",
        "\n",
        "  columns = np.array(texts[1].split(\" \"))\n",
        "  columns = columns[columns != \"\"]\n",
        "  df = []\n",
        "  for txt in texts[2:]:\n",
        "      txt = np.array(txt.split(\" \"))\n",
        "      txt = txt[txt!= \"\"]\n",
        "      df.append(txt)\n",
        "        \n",
        "  df = pd.DataFrame(df)\n",
        "  ui = pd.DataFrame(columns = columns)\n",
        "\n",
        "  if df.shape[1] == len(columns) + 1:\n",
        "     columns = [\"image_id\"]+ list(columns)\n",
        "  df.columns = columns  \n",
        "  df = df.dropna()\n",
        "\n",
        "  ## cast to integer\n",
        "  for nm in df.columns:\n",
        "      if nm != \"image_id\":\n",
        "          df[nm] = pd.to_numeric(df[nm],downcast=\"integer\")\n",
        "  return(df, ui)\n",
        "\n",
        "attributes, user_input = get_annotation()\n",
        "#print(attributes)\n",
        "#print(attributes['image_id'][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp4iFCiSCgPh",
        "colab_type": "text"
      },
      "source": [
        "The user input is stored in a vector (for now it is the encoded vector)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuVdwytNkfnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##test code\n",
        "\n",
        "#print(user_input)\n",
        "\n",
        "new_user_input = user_input.append(pd.Series([-1,1,-1,-1,-1,1,1,1,1,-1,1,-1,1,-1,1,-1,-1,1,1,-1, -1,1,1,1,1,1,1,1,-1,1,-1,1,1,-1,1,1,1,1,-1,1], index = user_input.columns), ignore_index=True)\n",
        "\n",
        "#print(new_user_input.loc[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRE7bQl0hGqw",
        "colab_type": "text"
      },
      "source": [
        "The images are taken from the website presented below. The data is stored in a list of size 202599. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fgTKsnJhSe9",
        "colab_type": "code",
        "outputId": "71af3f5d-c62b-4933-8db9-c5d852b50de0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!mkdir data_faces && wget https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-27 21:21:56--  https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip\n",
            "Resolving s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)... 52.219.120.152\n",
            "Connecting to s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)|52.219.120.152|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1443490838 (1.3G) [application/zip]\n",
            "Saving to: ‘celeba.zip’\n",
            "\n",
            "celeba.zip          100%[===================>]   1.34G  21.5MB/s    in 65s     \n",
            "\n",
            "2020-05-27 21:23:02 (21.1 MB/s) - ‘celeba.zip’ saved [1443490838/1443490838]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGrGeYkFitcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"celeba.zip\",\"r\") as zip_ref:\n",
        "  zip_ref.extractall(\"data_faces/\")\n",
        "\n",
        "root = 'data_faces/img_align_celeba'\n",
        "img_list = os.listdir(root)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwafX1trrCgh",
        "colab_type": "text"
      },
      "source": [
        "A function for displaying images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FabDQNt8uGyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show(img,renorm=False,nrow=8,interpolation='bicubic'):\n",
        "  if renorm:\n",
        "    img = img/2 + 0.5\n",
        "  img_grid = torchvision.utils.make_grid(img,nrow=nrow).numpy()\n",
        "  plt.figure()\n",
        "  plt.imshow(np.transpose(img_grid, (1,2,0)), interpolation=interpolation)\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub-nQht7vAxC",
        "colab_type": "text"
      },
      "source": [
        "The images are then cropped and resized to 64x64"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1smkyXyq7CW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "crop_size = 108\n",
        "re_size = 64\n",
        "offset_height = (238 - crop_size) // 2  #floor division\n",
        "offset_width = (178 - crop_size) // 2\n",
        "crop = lambda x: x[:, offset_height:offset_height + crop_size, offset_width:offset_width + crop_size]\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Lambda(crop),\n",
        "     transforms.ToPILImage(),\n",
        "     transforms.Resize((re_size, re_size), interpolation=Image.BICUBIC),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize(mean = [0.5]*3, std = [0.5]*3)\n",
        "     ])\n",
        "\n",
        "batch_size = 10\n",
        "celeba_data = datasets.ImageFolder('./data_faces', transform=transform)\n",
        "data_loader = DataLoader(celeba_data, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "path, ceva = celeba_data.imgs[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0m8MGXJue27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_it = iter(data_loader)\n",
        "\n",
        "database_size = 1000\n",
        "\n",
        "#batch, _ = next(batch_it)\n",
        "#show(next(batch_it), renorm = True, nrow = 4)\n",
        "#batch, _ = next(batch_it)\n",
        "#show(batch[0:3], renorm = True, nrow = 4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egt9O2xdoZ5p",
        "colab_type": "text"
      },
      "source": [
        "A class that holds all data regarding images (image, attributes and label). These are stored in three dictionaries: one for label-attributes, one for label-image and one for index-label. The index is needed in order to ensure that the values are correctly selected during the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScoO_2FDoNEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImageDictionary():\n",
        "  def __init__(self):\n",
        "    self.attributes = dict()\n",
        "    self.images = dict()\n",
        "    self.labels = dict()\n",
        "  \n",
        "  def addAttributes(self, key, attribute):\n",
        "    self.attributes[key] = attribute\n",
        "\n",
        "  def addImage(self, key, image):\n",
        "    #print('whyyy but with image ')\n",
        "    #print(image)\n",
        "    #print(key)\n",
        "    self.images[key] = image\n",
        "\n",
        "  def addLabel(self, key, label):\n",
        "    self.labels[key] = label\n",
        "\n",
        "  def getAttributes(self, key):\n",
        "    return self.attributes[key]\n",
        "\n",
        "  def getImage(self, key):\n",
        "    return self.images[key]\n",
        "\n",
        "  def getLabel(self, key):\n",
        "    return self.labels[key]\n",
        "\n",
        "  def printLabelDictionary(self):\n",
        "    print(\"index - label\")\n",
        "    for x, y in self.labels.items():\n",
        "      print(x, y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45l4uc-H8OsI",
        "colab_type": "text"
      },
      "source": [
        "A method for initializing the dictionary that will hold all attributes.\n",
        "This has to be done before the images are added since the attributes are taken from the database one time (first thing when the program is run). The images will be added later, in batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTkOt4fN8Zmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = ImageDictionary()\n",
        "\n",
        "for i in range(0, database_size):\n",
        "  attributes_list = [1]*40\n",
        "  label = attributes['image_id'][i]\n",
        "  #print(label)\n",
        "  attributes_list[0] = attributes['5_o_Clock_Shadow'][i]\n",
        "  attributes_list[1] = attributes['Arched_Eyebrows'][i]\n",
        "  attributes_list[2] = attributes['Attractive'][i]\n",
        "  attributes_list[3] = attributes['Bags_Under_Eyes'][i]\n",
        "  attributes_list[4] = attributes['Bald'][i]\n",
        "  attributes_list[5] = attributes['Bangs'][i]\n",
        "  attributes_list[6] = attributes['Big_Lips'][i]\n",
        "  attributes_list[7] = attributes['Big_Nose'][i]\n",
        "  attributes_list[8] = attributes['Black_Hair'][i]\n",
        "  attributes_list[9] = attributes['Blond_Hair'][i]\n",
        "  attributes_list[10] = attributes['Blurry'][i]\n",
        "  attributes_list[11] = attributes['Brown_Hair'][i]\n",
        "  attributes_list[12] = attributes['Bushy_Eyebrows'][i]\n",
        "  attributes_list[13] = attributes['Chubby'][i]\n",
        "  attributes_list[14] = attributes['Double_Chin'][i]\n",
        "  attributes_list[15] = attributes['Eyeglasses'][i]\n",
        "  attributes_list[16] = attributes['Goatee'][i]\n",
        "  attributes_list[17] = attributes['Gray_Hair'][i]\n",
        "  attributes_list[18] = attributes['Heavy_Makeup'][i]\n",
        "  attributes_list[19] = attributes['High_Cheekbones'][i]\n",
        "  attributes_list[20] = attributes['Male'][i]\n",
        "  attributes_list[21] = attributes['Mouth_Slightly_Open'][i]\n",
        "  attributes_list[22] = attributes['Mustache'][i]\n",
        "  attributes_list[23] = attributes['Narrow_Eyes'][i]\n",
        "  attributes_list[24] = attributes['No_Beard'][i]\n",
        "  attributes_list[25] = attributes['Oval_Face'][i]\n",
        "  attributes_list[26] = attributes['Pale_Skin'][i]\n",
        "  attributes_list[27] = attributes['Pointy_Nose'][i]\n",
        "  attributes_list[28] = attributes['Receding_Hairline'][i]\n",
        "  attributes_list[29] = attributes['Rosy_Cheeks'][i]\n",
        "  attributes_list[30] = attributes['Sideburns'][i]\n",
        "  attributes_list[31] = attributes['Smiling'][i]\n",
        "  attributes_list[32] = attributes['Straight_Hair'][i]\n",
        "  attributes_list[33] = attributes['Wavy_Hair'][i]\n",
        "  attributes_list[34] = attributes['Wearing_Earrings'][i]\n",
        "  attributes_list[35] = attributes['Wearing_Hat'][i]\n",
        "  attributes_list[36] = attributes['Wearing_Lipstick'][i]\n",
        "  attributes_list[37] = attributes['Wearing_Necklace'][i]\n",
        "  attributes_list[38] = attributes['Wearing_Necktie'][i]\n",
        "  attributes_list[39] = attributes['Young'][i]\n",
        "  dictionary.addAttributes(label, attributes_list)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRrq76RbD1hE",
        "colab_type": "text"
      },
      "source": [
        "This code adds the images to the dictionary.\n",
        "The images are added in batches of 10, hence the index formula **10*iteration + i** , where iteration is the training iteration (the index of the current batch in the original dataset)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G2NbA6Wo1uD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_data(data, iteration):\n",
        "  for i in range(0, len(data)):\n",
        "    path, _ = celeba_data.imgs[10*iteration + i]\n",
        "    label = path[-10:]\n",
        "    dictionary.addImage(label, data[i])\n",
        "    dictionary.addLabel(10*iteration + i, label)\n",
        "    #print('initialize_data label ' + label)\n",
        "    #show(batch[i], renorm = True, nrow = 4)\n",
        "  #image_data = ImageData(image_dictionary, attributes)\n",
        "  #list_attr = image_data.get_corresponding_attributes('000001.jpg')\n",
        "  #print(list_attr.loc[0])\n",
        "  #return image_dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogV6r15Z8UkK",
        "colab_type": "text"
      },
      "source": [
        "The Discriminator is based on the structure in the image:\n",
        "![alt text](https://i.stack.imgur.com/JH24l.png)\n",
        "\n",
        " and this article on [cGANs](https://machinelearningmastery.com/how-to-develop-a-conditional-generative-adversarial-network-from-scratch/)\n",
        "\n",
        "![Discriminator](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/05/Plot-of-the-Discriminator-Model-in-the-Conditional-Generative-Adversarial-Network-664x1024.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E38uKpbIWR-R",
        "colab_type": "text"
      },
      "source": [
        "nn.Conv2d:\n",
        "<br>\n",
        "\n",
        "\n",
        "*   in_channel -> equal to the number of neurons in the previous layer. Initial value is the number of channels of the image (in this case, 3 - coloured image)\n",
        "*   out_channel -> the number of important attributes we want to find\n",
        "*   kernel_size -> size of filter\n",
        "*   stride, padding ( (kernel_size - 1)/2 )\n",
        "\n",
        "\n",
        "Relu -> activation function (the neuron is fired or not)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGTsSJLjHDbJ",
        "colab_type": "text"
      },
      "source": [
        "nn.BCELoss() -> Binary Cross Entropy (we need a sigmoid layer in the \n",
        "network) - output belongs to [0,1]\n",
        "<br>\n",
        "Cross-entropy quantifies the difference between two probability distributions.\n",
        "<br>\n",
        "nn.MSELoss() -> Mean Squared Error (input and output have to have the same size and be float)\n",
        "<br>\n",
        "nn.L1Loss() -> Mean Absolute Error (input and output have to have the same size and be float)\n",
        "\n",
        "\n",
        "Taken from [Pytorch basics - intro to dataloaders and loss functions](https://towardsdatascience.com/pytorch-basics-intro-to-dataloaders-and-loss-functions-868e86450047)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJcWsOyk8U8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_discriminator(in_shape=(64,64,3), n_attributes=40):\n",
        "  \n",
        "  #input layer (from the image)\n",
        "  in_attribute = Input(shape=(40,))\n",
        "\n",
        "  #print(\"discriminator\")\n",
        "  #print(in_attribute.shape)\n",
        "\n",
        "  #embedding\n",
        "  in_attribute_embedding = Embedding(n_attributes, 67)(in_attribute)\n",
        "  #dense - scale up to image dimensions (40,64*64*3)\n",
        "\n",
        "  #print(\"discriminator\")\n",
        "  #print(in_attribute_embedding.shape)\n",
        "\n",
        "  number_nodes = in_shape[0]*in_shape[1]\n",
        "  in_attribute_embedding = Dense(number_nodes)(in_attribute_embedding)\n",
        "  #print(\"discriminator\")\n",
        "  #print(in_attribute_embedding.shape)\n",
        "\n",
        "  #reshape - (3,64,64)\n",
        "  in_attribute_embedding = Reshape((in_shape[0], in_shape[1], 40))(in_attribute_embedding)\n",
        "\n",
        "  #add the input image\n",
        "  in_image = Input(shape=in_shape)\n",
        "  \n",
        "  #concatenation step -> will add a new channel to the image\n",
        "  merged = Concatenate()([in_image, in_attribute_embedding])\n",
        "\n",
        "  #print(\"discriminator\")\n",
        "  #print(merged.shape)\n",
        "\n",
        "  #1st layer\n",
        "  #conv2D -> downsample from (64,64,4) to (32,32,256)\n",
        "  downsampled_image = Conv2D(128, (3,3), strides=(2,2), padding='same')(merged)\n",
        "  downsampled_image = LeakyReLU(alpha=0.2)(downsampled_image)\n",
        "\n",
        "  #print(\"layer 1\")\n",
        "  #print(downsampled_image.shape)\n",
        "\n",
        "  #2nd layer\n",
        "  #conv2D -> downsample from (32,32,256) to (16,16,256)\n",
        "  downsampled_image = Conv2D(128, (3,3), strides=(2,2), padding='same')(downsampled_image)\n",
        "  downsampled_image = LeakyReLU(alpha=0.2)(downsampled_image)\n",
        "\n",
        "  #print(\"layer 2\")\n",
        "  #print(downsampled_image.shape)\n",
        "\n",
        "  #3rd layer\n",
        "  #conv2D -> downsample from (16,16,256) to (8,8,256)\n",
        "  downsampled_image = Conv2D(128, (3,3), strides=(2,2), padding='same')(downsampled_image)\n",
        "  downsampled_image = LeakyReLU(alpha=0.2)(downsampled_image)\n",
        "\n",
        "\n",
        "  #print(\"layer 3\")\n",
        "  #print(downsampled_image.shape)\n",
        "\n",
        "  #flatten to (8*8*128)\n",
        "  flattened_image = Flatten()(downsampled_image)\n",
        "\n",
        "  #print(\"flattened\")\n",
        "  #print(flattened_image.shape)\n",
        "\n",
        "  #dropout -> no change -> remove unnecesarry nodes\n",
        "  flattened_image = Dropout(0.4)(flattened_image)\n",
        "\n",
        "  #dense -> output layer\n",
        "  out_layer = Dense(1, activation='sigmoid')(flattened_image)\n",
        "\n",
        "  #print(\"flattened\")\n",
        "  #print(out_layer.shape)\n",
        "\n",
        "  #build the model -> add optimizer, loss function and metric\n",
        "  model = Model([in_image, in_attribute], out_layer)\n",
        "\n",
        "  #print(\"!!!!!!!!!!!!!\")\n",
        "\n",
        "  opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "  #print(\"?????????\")\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW7Gxl-81S-_",
        "colab_type": "text"
      },
      "source": [
        "The Grenerator was inspired by the same paper as the Discriminator\n",
        "\n",
        "![Generator](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/05/Plot-of-the-Generator-Model-in-the-Conditional-Generative-Adversarial-Network-806x1024.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZpxJL_d1jqf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_generator(latent_dimension, n_attributes=40):\n",
        "  #input layer (from the image)\n",
        "  in_attribute = Input(shape=(40,))\n",
        "  #embedding\n",
        "  in_attribute_embedded = Embedding(n_attributes, 67)(in_attribute)\n",
        "  \n",
        "  #linear_multiplication\n",
        "  number_nodes = 8*8\n",
        "  in_attribute_embedded = Dense(number_nodes)(in_attribute_embedded)\n",
        "\n",
        "  #print(in_attribute_embedded.shape)\n",
        "\n",
        "  #reshape to (8,8,3)\n",
        "  in_attribute_embedded = Reshape((8,8,40))(in_attribute_embedded)\n",
        "\n",
        "  #print(in_attribute_embedded.shape)\n",
        "\n",
        "  #generator input\n",
        "  in_lat = Input(shape=(latent_dimension,))\n",
        "\n",
        "  #print(in_lat.shape)\n",
        "\n",
        "  #8x8 image\n",
        "  number_nodes = 8*8*128\n",
        "  gen = Dense(number_nodes)(in_lat)\n",
        "  gen = LeakyReLU(alpha=0.2)(gen)\n",
        "  gen = Reshape((8,8,128))(gen)\n",
        "\n",
        "  #print(in_attribute_embedded.shape)\n",
        "  #print(gen.shape)\n",
        "\n",
        "  #merge image gen and input attribute\n",
        "  merge = Concatenate()([gen, in_attribute_embedded])\n",
        "\n",
        "  #print(\"merge\")\n",
        "  #print(merge.shape)\n",
        "\n",
        "  #upsample to (16,16)\n",
        "  gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(merge)\n",
        "  gen = LeakyReLU(alpha=0.2)(gen)\n",
        "\n",
        "  #upsample to (32,32)\n",
        "  gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
        "  gen = LeakyReLU(alpha=0.2)(gen)\n",
        " \n",
        "  #upsample to (64,64)\n",
        "  gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
        "  gen = LeakyReLU(alpha=0.2)(gen)\n",
        " \n",
        "  #output layer\n",
        "  out_layer = Conv2D(3, (8,8), activation='tanh', padding='same')(gen)\n",
        " \n",
        "  #model\n",
        "  model = Model([in_lat, in_attribute], out_layer)\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKLd5QKFH6Jy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_gan(generator_model, discriminator_model):\n",
        "  discriminator_model.trainable = False\n",
        "  generator_noise, generator_attributes = generator_model.input\n",
        "  generator_output = generator_model.output\n",
        "\n",
        "  #print(\"gan\")\n",
        "  #print(generator_output.shape)\n",
        "\n",
        "  gan_output = discriminator_model([generator_output, generator_attributes])\n",
        "\n",
        "  model = Model([generator_noise, generator_attributes], gan_output)\n",
        "\n",
        "  opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmdWH5iJCf5D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_real_data():\n",
        "  \n",
        "  for i in range(0, int(database_size/10)):\n",
        "    batch, _ = next(batch_it)\n",
        "    initialize_data(batch, i)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PEx3Nj2AL2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_real_samples(db_size, number_samples):\n",
        "  images = []\n",
        "  attributes_list = []\n",
        "  \n",
        "  for i in range(0, number_samples):\n",
        "    label = dictionary.getLabel(random.randint(0, db_size - 1))\n",
        "    images.append(dictionary.getImage(label))\n",
        "    attributes_list.append(dictionary.getAttributes(label))\n",
        "\n",
        "\n",
        "  y = ones((number_samples, 1))\n",
        "\n",
        "  return [images, attributes_list], y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NzIwxaVFTXx",
        "colab_type": "text"
      },
      "source": [
        "generate points in latent space and reshape them into a batch of inputs for the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXdBh_8ED5Vq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_random_attribute_list(number_samples):\n",
        "  attributes_list = []\n",
        "  for i in range(0, number_samples):\n",
        "    attributes = []\n",
        "\n",
        "    for j in range(0, 40):\n",
        "      attributes.append(random.choice([-1,1]))\n",
        "\n",
        "    attributes_list.append(attributes)\n",
        "  \n",
        "  return attributes_list\n",
        "\n",
        "def generate_noise(latent_dim, number_samples, n_classes = 40):\n",
        "  #print(latent_dim)\n",
        "  x_input = randn(latent_dim * number_samples)\n",
        "  z_input = x_input.reshape(number_samples, latent_dim)\n",
        "  attributes = generate_random_attribute_list(number_samples)\n",
        "\n",
        "  return [z_input, attributes]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wa00h4909vCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_fake_data(generator, latent_dim, number_samples):\n",
        "  z_input, attribute_input = generate_noise(latent_dim, number_samples)\n",
        "  #print(\"generate_fake_data\")\n",
        "  #print(z_input.shape)\n",
        "  #print(np.array(attribute_input).shape)\n",
        "  images = generator.predict([np.array(z_input), np.array(attribute_input)])\n",
        "\n",
        "  y = zeros((number_samples, 1))\n",
        "  return [images, attribute_input], y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e7G9dO9-bWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(generator_model, discriminator_model, gan_model, \n",
        "          latent_dim, number_epochs = 100, number_batch = 256):\n",
        "  \n",
        "  batches_per_epoch = int(database_size / number_batch)\n",
        "  half_batch = int(number_batch / 2)\n",
        "\n",
        "  for epoch in range(number_epochs):\n",
        "    for batch_index in range(batches_per_epoch):\n",
        "\n",
        "      #print(\"aici\")\n",
        "\n",
        "      #get real data\n",
        "      [x_real, attributes_real], y_real = generate_real_samples(database_size, half_batch)\n",
        "\n",
        "      #print(\"aici 2\")\n",
        "\n",
        "      x_real_tensors_swapped = [tf.transpose(x, [1, 2, 0]) for x in x_real]\n",
        "      x_real_array = [x.numpy() for x in x_real_tensors_swapped]\n",
        "\n",
        "      #print(x_real_array[0].shape)\n",
        "      #print(np.array(attributes_real).shape)\n",
        "      #print(np.array(y_real).shape)\n",
        "\n",
        "      d_loss1, _ = discriminator_model.train_on_batch([x_real_array, attributes_real], y_real)\n",
        "\n",
        "      #print(\"aici 3\")\n",
        "      #print(latent_dim)\n",
        "\n",
        "      #get fake data\n",
        "      [x_fake, attributes], y_fake = generate_fake_data(generator_model, latent_dim, half_batch)\n",
        "\n",
        "      #print(\"aici 4\")\n",
        "\n",
        "      d_loss2, _ = discriminator_model.train_on_batch([np.array(x_fake), np.array(attributes)], np.array(y_fake))\n",
        "\n",
        "      #get noise\n",
        "      [z_input, attributes_input] = generate_noise(latent_dim, number_batch)\n",
        "\n",
        "      y_gan = ones((number_batch, 1))\n",
        "\n",
        "      g_loss = gan_model.train_on_batch([np.array(z_input), np.array(attributes_input)], np.array(y_gan))\n",
        "\n",
        "      #summarise loss on this batch\n",
        "      print('> epoch: %d, (batch_index/batches_per_epoch): %d/%d, d1=%.3f, d2=%.3f g=%.3f' % \n",
        "            (epoch + 1, batch_index, batches_per_epoch, d_loss1, d_loss2, g_loss))\n",
        "  generator_model.save('cgan_generator.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDwgu0OAHmqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent_dim = 100\n",
        "\n",
        "discriminator_model = create_discriminator()\n",
        "generator_model = create_generator(latent_dim)\n",
        "gan_model = create_gan(generator_model, discriminator_model)\n",
        "\n",
        "dataset = load_real_data()\n",
        "\n",
        "train(generator_model, discriminator_model, gan_model, latent_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP7GlvCQ7FFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_image_result(results, n):\n",
        "  for i in range(n*n):\n",
        "    pyplot.subplot(n, n, 1+i)\n",
        "    pyplot.axis('off')\n",
        "    pyplot.imshow(results[i,:,:,0])\n",
        "  pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSTa9Ue372P4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = generator_model\n",
        "\n",
        "noise, _ = generate_noise(100,100)\n",
        "attributes = []\n",
        "\n",
        "for i in range ()\n",
        "  attributes.append([-1,1,-1,-1,-1,1,1,1,1,-1,1,-1,1,-1,1,-1,-1,1,1,-1, -1,1,1,1,1,1,1,1,-1,1,-1,1,1,-1,1,1,1,1,-1,1])\n",
        "\n",
        "X = model.predict(np.array(noise), np.array(attributes))\n",
        "X = (X+1)/2.0\n",
        "display_image_result(X,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN7JuocswSza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}