{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text to Image Synthesis",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMuOmeyOgwnqpkzS2aAZhFW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ionas-emanuela/Text-to-image-synthesis/blob/master/Text_to_Image_Synthesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPTkHQ-ma8ak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torch===1.2.0 torchvision===0.4.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip3 install torchvision\n",
        "!pip install tensorboardX"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NITN6j-vqu2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfXa9AxzolLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp5f8K4Zemq-",
        "colab_type": "text"
      },
      "source": [
        "All the necessary imports are here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjPlq0qvzEQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl3lKMrjefYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn, optim\n",
        "from torch.autograd.variable import Variable\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import errno\n",
        "import torchvision.utils as vutils\n",
        "from tensorboardX import SummaryWriter\n",
        "from IPython import display\n",
        "import PIL.Image as Image\n",
        "from matplotlib import pyplot as plt\n",
        "import utils\n",
        "\n",
        "import random\n",
        "\n",
        "from numpy import expand_dims\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy import asarray\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from keras.datasets.fashion_mnist import load_data\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Concatenate\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sJJAfCZbNAo",
        "colab_type": "text"
      },
      "source": [
        "Install the necessary libraries for connecting to google drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAd6XC23bKGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_fZ7ZgkbLHy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_n26L_AbvXe",
        "colab_type": "text"
      },
      "source": [
        "The attributes are taken from here [list_attr_celeba.txt](https://drive.google.com/open?id=0B7EVK8r0v71pblRyaVFSWGxPY0U).\n",
        "They are presented as a table of 41 x 202599. The columns are the image_id + attributes and each row has a value of 1 or -1 (if the attribute is found or not in the image).\n",
        "\n",
        "---\n",
        "\n",
        "The attributes are the following (in this exact order) : ['image_id', '5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open','Mustache', 'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace', 'Wearing_Necktie', 'Young']\n",
        "\n",
        "---\n",
        "\n",
        "Note that these values are saved in a DataFrame, so the indexing is done using .loc['image_id'], this will retrieve the row of data corresponding to a certain image.\n",
        "\n",
        "---\n",
        "The one-hot encoding vector obtained from the textual input is stored in a DataFrame of 40 x 1, the only occupied row being row 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVgZHLgtbq0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_annotation():\n",
        "  downloaded = drive.CreateFile({'id':'0B7EVK8r0v71pblRyaVFSWGxPY0U'})\n",
        "  downloaded.GetContentFile('list_attr_celeba.txt')\n",
        "  rfile = open('list_attr_celeba.txt', 'r')\n",
        "  texts = rfile.read().split(\"\\n\")\n",
        "  rfile.close()\n",
        "\n",
        "  columns = np.array(texts[1].split(\" \"))\n",
        "  columns = columns[columns != \"\"]\n",
        "  df = []\n",
        "  for txt in texts[2:]:\n",
        "      txt = np.array(txt.split(\" \"))\n",
        "      txt = txt[txt!= \"\"]\n",
        "      df.append(txt)\n",
        "        \n",
        "  df = pd.DataFrame(df)\n",
        "  ui = pd.DataFrame(columns = columns)\n",
        "\n",
        "  if df.shape[1] == len(columns) + 1:\n",
        "     columns = [\"image_id\"]+ list(columns)\n",
        "  df.columns = columns  \n",
        "  df = df.dropna()\n",
        "\n",
        "  ## cast to integer\n",
        "  for nm in df.columns:\n",
        "      if nm != \"image_id\":\n",
        "          df[nm] = pd.to_numeric(df[nm],downcast=\"integer\")\n",
        "  return(df, ui)\n",
        "\n",
        "attributes, user_input = get_annotation()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRE7bQl0hGqw",
        "colab_type": "text"
      },
      "source": [
        "The images are taken from the website presented below. The data is stored in a list of size 202599. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fgTKsnJhSe9",
        "colab_type": "code",
        "outputId": "04330217-8366-420a-dfec-68caec70bff9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!mkdir data_faces && wget https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip "
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data_faces’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGrGeYkFitcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"celeba.zip\",\"r\") as zip_ref:\n",
        "  zip_ref.extractall(\"data_faces/\")\n",
        "\n",
        "root = 'data_faces/img_align_celeba'\n",
        "img_list = os.listdir(root)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub-nQht7vAxC",
        "colab_type": "text"
      },
      "source": [
        "The images are then cropped and resized to 64x64"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1smkyXyq7CW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "crop_size = 108\n",
        "re_size = 64\n",
        "offset_height = (238 - crop_size) // 2  #floor division\n",
        "offset_width = (178 - crop_size) // 2\n",
        "crop = lambda x: x[:, offset_height:offset_height + crop_size, offset_width:offset_width + crop_size]\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Lambda(crop),\n",
        "     transforms.ToPILImage(),\n",
        "     transforms.Resize((re_size, re_size), interpolation=Image.BICUBIC),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize(mean = [0.5]*3, std = [0.5]*3)\n",
        "     ])\n",
        "\n",
        "batch_size = 10\n",
        "celeba_data = datasets.ImageFolder('./data_faces', transform=transform)\n",
        "data_loader = DataLoader(celeba_data, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "path, ceva = celeba_data.imgs[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0m8MGXJue27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_it = iter(data_loader)\n",
        "\n",
        "database_size = 10000\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egt9O2xdoZ5p",
        "colab_type": "text"
      },
      "source": [
        "A class that holds all data regarding images (image, attributes and label). These are stored in three dictionaries: one for label-attributes, one for label-image and one for index-label. The index is needed in order to ensure that the values are correctly selected during the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScoO_2FDoNEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImageDictionary():\n",
        "  def __init__(self):\n",
        "    self.attributes = dict()\n",
        "    self.images = dict()\n",
        "    self.labels = dict()\n",
        "  \n",
        "  def addAttributes(self, key, attribute):\n",
        "    self.attributes[key] = attribute\n",
        "\n",
        "  def addImage(self, key, image):\n",
        "    self.images[key] = image\n",
        "\n",
        "  def addLabel(self, key, label):\n",
        "    self.labels[key] = label\n",
        "\n",
        "  def getAttributes(self, key):\n",
        "    return self.attributes[key]\n",
        "\n",
        "  def getImage(self, key):\n",
        "    return self.images[key]\n",
        "\n",
        "  def getLabel(self, key):\n",
        "    return self.labels[key]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45l4uc-H8OsI",
        "colab_type": "text"
      },
      "source": [
        "A method for initializing the dictionary that will hold all attributes.\n",
        "This has to be done before the images are added since the attributes are taken from the database one time (first thing when the program is run). The images will be added later, in batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTkOt4fN8Zmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = ImageDictionary()\n",
        "\n",
        "for i in range(0, database_size):\n",
        "  attributes_list = [1]*40\n",
        "  label = attributes['image_id'][i]\n",
        "  \n",
        "  attributes_list[0] = attributes['5_o_Clock_Shadow'][i]\n",
        "  attributes_list[1] = attributes['Arched_Eyebrows'][i]\n",
        "  attributes_list[2] = attributes['Attractive'][i]\n",
        "  attributes_list[3] = attributes['Bags_Under_Eyes'][i]\n",
        "  attributes_list[4] = attributes['Bald'][i]\n",
        "  attributes_list[5] = attributes['Bangs'][i]\n",
        "  attributes_list[6] = attributes['Big_Lips'][i]\n",
        "  attributes_list[7] = attributes['Big_Nose'][i]\n",
        "  attributes_list[8] = attributes['Black_Hair'][i]\n",
        "  attributes_list[9] = attributes['Blond_Hair'][i]\n",
        "  attributes_list[10] = attributes['Blurry'][i]\n",
        "  attributes_list[11] = attributes['Brown_Hair'][i]\n",
        "  attributes_list[12] = attributes['Bushy_Eyebrows'][i]\n",
        "  attributes_list[13] = attributes['Chubby'][i]\n",
        "  attributes_list[14] = attributes['Double_Chin'][i]\n",
        "  attributes_list[15] = attributes['Eyeglasses'][i]\n",
        "  attributes_list[16] = attributes['Goatee'][i]\n",
        "  attributes_list[17] = attributes['Gray_Hair'][i]\n",
        "  attributes_list[18] = attributes['Heavy_Makeup'][i]\n",
        "  attributes_list[19] = attributes['High_Cheekbones'][i]\n",
        "  attributes_list[20] = attributes['Male'][i]\n",
        "  attributes_list[21] = attributes['Mouth_Slightly_Open'][i]\n",
        "  attributes_list[22] = attributes['Mustache'][i]\n",
        "  attributes_list[23] = attributes['Narrow_Eyes'][i]\n",
        "  attributes_list[24] = attributes['No_Beard'][i]\n",
        "  attributes_list[25] = attributes['Oval_Face'][i]\n",
        "  attributes_list[26] = attributes['Pale_Skin'][i]\n",
        "  attributes_list[27] = attributes['Pointy_Nose'][i]\n",
        "  attributes_list[28] = attributes['Receding_Hairline'][i]\n",
        "  attributes_list[29] = attributes['Rosy_Cheeks'][i]\n",
        "  attributes_list[30] = attributes['Sideburns'][i]\n",
        "  attributes_list[31] = attributes['Smiling'][i]\n",
        "  attributes_list[32] = attributes['Straight_Hair'][i]\n",
        "  attributes_list[33] = attributes['Wavy_Hair'][i]\n",
        "  attributes_list[34] = attributes['Wearing_Earrings'][i]\n",
        "  attributes_list[35] = attributes['Wearing_Hat'][i]\n",
        "  attributes_list[36] = attributes['Wearing_Lipstick'][i]\n",
        "  attributes_list[37] = attributes['Wearing_Necklace'][i]\n",
        "  attributes_list[38] = attributes['Wearing_Necktie'][i]\n",
        "  attributes_list[39] = attributes['Young'][i]\n",
        "  dictionary.addAttributes(label, attributes_list)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRrq76RbD1hE",
        "colab_type": "text"
      },
      "source": [
        "This code adds the images to the dictionary.\n",
        "The images are added in batches of 10, hence the index formula **10*iteration + i** , where iteration is the training iteration (the index of the current batch in the original dataset)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G2NbA6Wo1uD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_data(data, iteration):\n",
        "  for i in range(0, len(data)):\n",
        "    path, _ = celeba_data.imgs[10*iteration + i]\n",
        "    label = path[-10:]\n",
        "    dictionary.addImage(label, data[i])\n",
        "    dictionary.addLabel(10*iteration + i, label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogV6r15Z8UkK",
        "colab_type": "text"
      },
      "source": [
        "The Discriminator is based on this article on [cGANs](https://machinelearningmastery.com/how-to-develop-a-conditional-generative-adversarial-network-from-scratch/)\n",
        "\n",
        "![Discriminator](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/05/Plot-of-the-Discriminator-Model-in-the-Conditional-Generative-Adversarial-Network-664x1024.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E38uKpbIWR-R",
        "colab_type": "text"
      },
      "source": [
        "nn.Conv2d:\n",
        "<br>\n",
        "\n",
        "\n",
        "*   in_channel -> equal to the number of neurons in the previous layer. Initial value is the number of channels of the image (in this case, 3 - coloured image)\n",
        "*   out_channel -> the number of important attributes we want to find\n",
        "*   kernel_size -> size of filter\n",
        "*   stride, padding ( (kernel_size - 1)/2 )\n",
        "\n",
        "\n",
        "Relu -> activation function (the neuron is fired or not)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGTsSJLjHDbJ",
        "colab_type": "text"
      },
      "source": [
        "nn.BCELoss() -> Binary Cross Entropy (we need a sigmoid layer in the \n",
        "network) - output belongs to [0,1]\n",
        "<br>\n",
        "Cross-entropy quantifies the difference between two probability distributions.\n",
        "<br>\n",
        "nn.MSELoss() -> Mean Squared Error (input and output have to have the same size and be float)\n",
        "<br>\n",
        "nn.L1Loss() -> Mean Absolute Error (input and output have to have the same size and be float)\n",
        "\n",
        "\n",
        "Taken from [Pytorch basics - intro to dataloaders and loss functions](https://towardsdatascience.com/pytorch-basics-intro-to-dataloaders-and-loss-functions-868e86450047)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJcWsOyk8U8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_discriminator(in_shape=(64,64,3), n_attributes=40):\n",
        "  \n",
        "  #input layer (attributes)\n",
        "  in_attribute = Input(shape=(40,))\n",
        "\n",
        "  #embedding - (40,67)\n",
        "  in_attribute_embedding = Embedding(n_attributes, 67)(in_attribute)\n",
        "\n",
        "  #dense - scale up to image dimensions (64*64)\n",
        "  number_nodes = in_shape[0]*in_shape[1]\n",
        "  in_attribute_embedding = Dense(number_nodes)(in_attribute_embedding)\n",
        "\n",
        "  #reshape - (64,64,40)\n",
        "  in_attribute_embedding = Reshape((in_shape[0], in_shape[1], 40))(in_attribute_embedding)\n",
        "\n",
        "  #add the input image\n",
        "  in_image = Input(shape=in_shape)\n",
        "  \n",
        "  #concatenation step -> will add a new channel to the image (64,64,43)\n",
        "  merged = Concatenate()([in_image, in_attribute_embedding])\n",
        "\n",
        "  #1st layer\n",
        "  #conv2D -> downsample from (64,64,43) to (32,32,128)\n",
        "  downsampled_image = Conv2D(128, (3,3), strides=(2,2), padding='same')(merged)\n",
        "  downsampled_image = LeakyReLU(alpha=0.2)(downsampled_image)\n",
        "\n",
        "  #2nd layer\n",
        "  #conv2D -> downsample from (32,32,128) to (16,16,128)\n",
        "  downsampled_image = Conv2D(128, (3,3), strides=(2,2), padding='same')(downsampled_image)\n",
        "  downsampled_image = LeakyReLU(alpha=0.2)(downsampled_image)\n",
        "\n",
        "  #3rd layer\n",
        "  #conv2D -> downsample from (16,16,128) to (8,8,128)\n",
        "  downsampled_image = Conv2D(128, (3,3), strides=(2,2), padding='same')(downsampled_image)\n",
        "  downsampled_image = LeakyReLU(alpha=0.2)(downsampled_image)\n",
        "\n",
        "  #flatten to (8*8*128)\n",
        "  flattened_image = Flatten()(downsampled_image)\n",
        "\n",
        "  #dropout -> no change -> remove unnecesarry nodes\n",
        "  flattened_image = Dropout(0.4)(flattened_image)\n",
        "\n",
        "  #dense -> output layer\n",
        "  out_layer = Dense(1, activation='sigmoid')(flattened_image)\n",
        "\n",
        "  #build the model -> add optimizer, loss function and metric\n",
        "  model = Model([in_image, in_attribute], out_layer)\n",
        "\n",
        "  opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW7Gxl-81S-_",
        "colab_type": "text"
      },
      "source": [
        "The Grenerator was inspired by the same paper as the Discriminator\n",
        "\n",
        "![Generator](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/05/Plot-of-the-Generator-Model-in-the-Conditional-Generative-Adversarial-Network-806x1024.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZpxJL_d1jqf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_generator(latent_dimension, n_attributes=40):\n",
        "  #input layer (attributes)\n",
        "  in_attribute = Input(shape=(40,))\n",
        "\n",
        "  #embedding - (40,67)\n",
        "  in_attribute_embedded = Embedding(n_attributes, 67)(in_attribute)\n",
        "  \n",
        "  #linear_multiplication\n",
        "  number_nodes = 8*8\n",
        "  in_attribute_embedded = Dense(number_nodes)(in_attribute_embedded)\n",
        "\n",
        "  #reshape to (8,8,40)\n",
        "  in_attribute_embedded = Reshape((8,8,40))(in_attribute_embedded)\n",
        "\n",
        "  #generator input\n",
        "  in_lat = Input(shape=(latent_dimension,))\n",
        "\n",
        "  #8x8 image\n",
        "  number_nodes = 8*8*128\n",
        "  gen = Dense(number_nodes)(in_lat)\n",
        "  gen = LeakyReLU(alpha=0.2)(gen)\n",
        "  gen = Reshape((8,8,128))(gen)\n",
        "\n",
        "  #merge image gen and input attribute\n",
        "  merge = Concatenate()([gen, in_attribute_embedded])\n",
        "\n",
        "  #upsample to (16,16)\n",
        "  gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(merge)\n",
        "  gen = LeakyReLU(alpha=0.2)(gen)\n",
        "\n",
        "  #upsample to (32,32)\n",
        "  gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
        "  gen = LeakyReLU(alpha=0.2)(gen)\n",
        " \n",
        "  #upsample to (64,64)\n",
        "  gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
        "  gen = LeakyReLU(alpha=0.2)(gen)\n",
        " \n",
        "  #output layer\n",
        "  out_layer = Conv2D(3, (8,8), activation='tanh', padding='same')(gen)\n",
        " \n",
        "  #model\n",
        "  model = Model([in_lat, in_attribute], out_layer)\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKLd5QKFH6Jy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_gan(generator_model, discriminator_model):\n",
        "  discriminator_model.trainable = False\n",
        "  generator_noise, generator_attributes = generator_model.input\n",
        "  generator_output = generator_model.output\n",
        "\n",
        "  gan_output = discriminator_model([generator_output, generator_attributes])\n",
        "\n",
        "  model = Model([generator_noise, generator_attributes], gan_output)\n",
        "\n",
        "  opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmdWH5iJCf5D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_real_data():\n",
        "  \n",
        "  for i in range(0, int(database_size/10)):\n",
        "    batch, _ = next(batch_it)\n",
        "    initialize_data(batch, i)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PEx3Nj2AL2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_real_samples(db_size, number_samples):\n",
        "  images = []\n",
        "  attributes_list = []\n",
        "  \n",
        "  for i in range(0, number_samples):\n",
        "    label = dictionary.getLabel(random.randint(0, db_size - 1))\n",
        "    images.append(dictionary.getImage(label))\n",
        "    attributes_list.append(dictionary.getAttributes(label))\n",
        "\n",
        "  y = ones((number_samples, 1))\n",
        "\n",
        "  return [images, attributes_list], y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NzIwxaVFTXx",
        "colab_type": "text"
      },
      "source": [
        "generate points in latent space and reshape them into a batch of inputs for the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXdBh_8ED5Vq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_random_attribute_list(number_samples):\n",
        "  attributes_list = []\n",
        "  for i in range(0, number_samples):\n",
        "    attributes = []\n",
        "\n",
        "    for j in range(0, 40):\n",
        "      attributes.append(random.choice([-1,1]))\n",
        "\n",
        "    attributes_list.append(attributes)\n",
        "  \n",
        "  return attributes_list\n",
        "\n",
        "def generate_noise(latent_dim, number_samples, n_classes = 40):\n",
        "  x_input = randn(latent_dim * number_samples)\n",
        "  z_input = x_input.reshape(number_samples, latent_dim)\n",
        "  attributes = generate_random_attribute_list(number_samples)\n",
        "\n",
        "  return [z_input, attributes]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wa00h4909vCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_fake_data(generator, latent_dim, number_samples):\n",
        "  z_input, attribute_input = generate_noise(latent_dim, number_samples)\n",
        "  images = generator.predict([np.array(z_input), np.array(attribute_input)])\n",
        "\n",
        "  y = zeros((number_samples, 1))\n",
        "  return [images, attribute_input], y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e7G9dO9-bWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(generator_model, discriminator_model, gan_model, \n",
        "          latent_dim, number_epochs = 100, number_batch = 256):\n",
        "  \n",
        "  batches_per_epoch = int(database_size / number_batch)\n",
        "  half_batch = int(number_batch / 2)\n",
        "  d1_hist, d2_hist, g_hist = list(), list(), list()\n",
        "\n",
        "  for epoch in range(number_epochs):\n",
        "    for batch_index in range(batches_per_epoch):\n",
        "\n",
        "      #get real data\n",
        "      [x_real, attributes_real], y_real = generate_real_samples(database_size, half_batch)\n",
        "\n",
        "      x_real_tensors_swapped = [tf.transpose(x, [1, 2, 0]) for x in x_real]\n",
        "      x_real_array = [x.numpy() for x in x_real_tensors_swapped]\n",
        "\n",
        "      d_loss1, _ = discriminator_model.train_on_batch([x_real_array, attributes_real], y_real)\n",
        "\n",
        "      #get fake data\n",
        "      [x_fake, attributes], y_fake = generate_fake_data(generator_model, latent_dim, half_batch)\n",
        "\n",
        "      d_loss2, _ = discriminator_model.train_on_batch([np.array(x_fake), np.array(attributes)], np.array(y_fake))\n",
        "\n",
        "      #get noise for gan\n",
        "      [z_input, attributes_input] = generate_noise(latent_dim, number_batch)\n",
        "      y_gan = ones((number_batch, 1))\n",
        "      g_loss = gan_model.train_on_batch([np.array(z_input), np.array(attributes_input)], np.array(y_gan))\n",
        "\n",
        "      d1_hist.append(d_loss1)\n",
        "\t\t  d2_hist.append(d_loss2)\n",
        "\t\t  g_hist.append(g_loss)\n",
        "\n",
        "      #summarise loss on this batch\n",
        "      print('> epoch: %d, (batch_index/batches_per_epoch): %d/%d, d1=%.3f, d2=%.3f g=%.3f' % \n",
        "            (epoch + 1, batch_index, batches_per_epoch, d_loss1, d_loss2, g_loss))\n",
        "  generator_model.save('cgan_generator.h5')\n",
        "  plot_history(d1_hist, d2_hist, g_hist)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDwgu0OAHmqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent_dim = 100\n",
        "\n",
        "discriminator_model = create_discriminator()\n",
        "generator_model = create_generator(latent_dim)\n",
        "gan_model = create_gan(generator_model, discriminator_model)\n",
        "\n",
        "dataset = load_real_data()\n",
        "\n",
        "train(generator_model, discriminator_model, gan_model, latent_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1KFVrfSooXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_history(d1_hist, d2_hist, g_hist):\n",
        "\t# plot loss\n",
        "\tpyplot.subplot(2, 1, 1)\n",
        "\tpyplot.plot(d1_hist, label='d-real')\n",
        "\tpyplot.plot(d2_hist, label='d-fake')\n",
        "\tpyplot.plot(g_hist, label='gen')\n",
        "\tpyplot.legend()\n",
        "\t# save plot to file\n",
        "\tpyplot.savefig('results_collapse/plot_line_plot_loss.png')\n",
        "\tpyplot.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP7GlvCQ7FFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_image_result(results, n):\n",
        "  for i in range(n*n):\n",
        "    pyplot.subplot(n, n, 1+i)\n",
        "    pyplot.axis('off')\n",
        "    pyplot.imshow(results[i,:,:,0])\n",
        "  pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSTa9Ue372P4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = generator_model\n",
        "\n",
        "noise, _ = generate_noise(100,100)\n",
        "attributes = []\n",
        "\n",
        "for i in range(0,100):\n",
        "  attributes.append([-1,1,-1,-1,-1,1,1,1,1,-1,1,-1,1,-1,1,-1,-1,1,1,-1,-1,1,1,1,1,1,1,1,-1,-1,-1,1,1,-1,-1,-1,-1,-1,-1,-1])\n",
        "\n",
        "X = model.predict([np.array(noise), np.array(attributes)])\n",
        "X = (X+1)/2.0\n",
        "display_image_result(X,3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN7JuocswSza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}